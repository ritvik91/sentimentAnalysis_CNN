{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn import naive_bayes, svm, preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection.univariate_selection import chi2, SelectKBest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"C:/workspace/python/sentiment.analysisRitvik/sentiment.analysisRitvik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### Initialization #####################\n",
    "\n",
    "write_to_csv = False\n",
    "\n",
    "# term_vector_type = {\"TFIDF\" \"Word2vec\", \"Word2vec_pretrained\"}\n",
    "# {\"Word2vec\", \"Word2vec_pretrained\"}: Google word2vec representation {without, with} pre-trained models\n",
    "# Specify model_name if there's a pre-trained model to be loaded\n",
    "vector_type = \"TFIDF\"\n",
    "model_name = \"sentiment analysis MR.bin\"\n",
    "\n",
    "# model_type = {\"bin\", \"reg\"}\n",
    "# Specify whether pre-trained word2vec model is binary\n",
    "model_type = \"bin\"\n",
    "   \n",
    "# Parameters for word2vec\n",
    "# num_features need to be identical with the pre-trained model\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count to be included for training                      \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# training_model = {\"RF\", \"NB\", \"SVM\", \"no\"}\n",
    "training_model = \"NB\"\n",
    "\n",
    "# feature scaling = {\"standard\", \"signed\", \"unsigned\", \"no\"}\n",
    "# Note: Scaling is needed for SVM\n",
    "scaling = \"no\"\n",
    "\n",
    "# dimension reduction = {\"SVD\", \"chi2\", \"no\"}\n",
    "# Note: For NB models, we cannot perform truncated SVD as it will make input negative\n",
    "# chi2 is the feature selectioin based on chi2 independence test\n",
    "dim_reduce = \"chi2\"\n",
    "num_dim = 500\n",
    "\n",
    "##################### End of Initialization #####################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### Function Definition #####################\n",
    "\n",
    "def clean_review(raw_review, remove_stopwords = False, output_format = \"string\"):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            raw_review: raw text of a movie review\n",
    "            remove_stopwords: a boolean variable to indicate whether to remove stop words\n",
    "            output_format: if \"string\", return a cleaned string \n",
    "                           if \"list\", a list of words extracted from cleaned string.\n",
    "    Output:\n",
    "            Cleaned string or list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove HTML markup\n",
    "    text = BeautifulSoup(raw_review)\n",
    "    \n",
    "    # Keep only characters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text.get_text())\n",
    "    \n",
    "    # Split words and store to list\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "    \n",
    "        # Use set as it has O(1) lookup time\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in text if w not in stops]\n",
    "    \n",
    "    else:\n",
    "        words = text\n",
    "    \n",
    "    # Return a cleaned string or list\n",
    "    if output_format == \"string\":\n",
    "        return \" \".join(words)\n",
    "        \n",
    "    elif output_format == \"list\":\n",
    "        return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_doublelist(review, tokenizer, remove_stopwords = False):\n",
    "    \"\"\"\n",
    "    Function which generates a list of lists of words from a review for word2vec uses.\n",
    "    \n",
    "    Input:\n",
    "        review: raw text of a movie review\n",
    "        tokenizer: tokenizer for sentence parsing\n",
    "                   nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        remove_stopwords: a boolean variable to indicate whether to remove stop words\n",
    "    \n",
    "    Output:\n",
    "        A list of lists.\n",
    "        The outer list consists of all sentences in a review.\n",
    "        The inner list consists of all words in a sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a list of sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentence_list = []\n",
    "    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentence_list.append(clean_review(raw_sentence, False, \"list\"))         \n",
    "    return sentence_list\n",
    "\n",
    "\n",
    "def review_to_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Function which generates a feature vector for the given review.\n",
    "    \n",
    "    Input:\n",
    "        words: a list of words extracted from a review\n",
    "        model: trained word2vec model\n",
    "        num_features: dimension of word2vec vectors\n",
    "        \n",
    "    Output:\n",
    "        a numpy array representing the review\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_vec = np.zeros((num_features), dtype=\"float32\")\n",
    "    word_count = 0\n",
    "    \n",
    "    # index2word is a list consisting of all words in the vocabulary\n",
    "    # Convert list to set for speed\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            word_count += 1\n",
    "            feature_vec += model[word]\n",
    "\n",
    "    feature_vec /= word_count\n",
    "    return feature_vec\n",
    "    \n",
    "    \n",
    "def gen_review_vecs(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    Function which generates a m-by-n numpy array from all reviews,\n",
    "    where m is len(reviews), and n is num_feature\n",
    "    \n",
    "    Input:\n",
    "            reviews: a list of lists. \n",
    "                     Inner lists are words from each review.\n",
    "                     Outer lists consist of all reviews\n",
    "            model: trained word2vec model\n",
    "            num_feature: dimension of word2vec vectors\n",
    "    Output: m-by-n numpy array, where m is len(review) and n is num_feature\n",
    "    \"\"\"\n",
    "\n",
    "    curr_index = 0\n",
    "    review_feature_vecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "\n",
    "       if curr_index%1000 == 0.:\n",
    "           print \"Vectorizing review %d of %d\" % (curr_index, len(reviews))\n",
    "   \n",
    "       review_feature_vecs[curr_index] = review_to_vec(review, model, num_features)\n",
    "       curr_index += 1\n",
    "       \n",
    "    return review_feature_vecs\n",
    "    \n",
    "    \n",
    "##################### End of Function Definition #####################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### Main Program ###########################\n",
    "\n",
    "train_list = []\n",
    "test_list = []\n",
    "word2vec_input = []\n",
    "pred = []\n",
    "\n",
    "train_data = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=0)\n",
    "test_data = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if vector_type == \"Word2vec\":\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    from nltk import word_tokenize,sent_tokenize\n",
    "    unlab_train_data = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    logging.basicConfig(format='%(asctime)s: %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning training review 0\n",
      "Cleaning training review 1000\n",
      "Cleaning training review 2000\n",
      "Cleaning training review 3000\n",
      "Cleaning training review 4000\n",
      "Cleaning training review 5000\n",
      "Cleaning training review 6000\n",
      "Cleaning training review 7000\n",
      "Cleaning training review 8000\n",
      "Cleaning training review 9000\n",
      "Cleaning training review 10000\n",
      "Cleaning training review 11000\n",
      "Cleaning training review 12000\n",
      "Cleaning training review 13000\n",
      "Cleaning training review 14000\n",
      "Cleaning training review 15000\n",
      "Cleaning training review 16000\n",
      "Cleaning training review 17000\n",
      "Cleaning training review 18000\n",
      "Cleaning training review 19000\n",
      "Cleaning training review 20000\n",
      "Cleaning training review 21000\n",
      "Cleaning training review 22000\n",
      "Cleaning training review 23000\n",
      "Cleaning training review 24000\n",
      "Cleaning test review 0\n",
      "Cleaning test review 1000\n",
      "Cleaning test review 2000\n",
      "Cleaning test review 3000\n",
      "Cleaning test review 4000\n",
      "Cleaning test review 5000\n",
      "Cleaning test review 6000\n",
      "Cleaning test review 7000\n",
      "Cleaning test review 8000\n",
      "Cleaning test review 9000\n",
      "Cleaning test review 10000\n",
      "Cleaning test review 11000\n",
      "Cleaning test review 12000\n",
      "Cleaning test review 13000\n",
      "Cleaning test review 14000\n",
      "Cleaning test review 15000\n",
      "Cleaning test review 16000\n",
      "Cleaning test review 17000\n",
      "Cleaning test review 18000\n",
      "Cleaning test review 19000\n",
      "Cleaning test review 20000\n",
      "Cleaning test review 21000\n",
      "Cleaning test review 22000\n",
      "Cleaning test review 23000\n",
      "Cleaning test review 24000\n"
     ]
    }
   ],
   "source": [
    "# Extract words from reviews\n",
    "# xrange is faster when iterating\n",
    "if vector_type == \"Word2vec\" or vector_type == \"Word2vec_pretrained\":\n",
    "    \n",
    "    for i in xrange(0, len(train_data.review)):\n",
    "        \n",
    "        if vector_type == \"Word2vec\":\n",
    "            # Decode utf-8 coding first\n",
    "            word2vec_input.extend(review_to_doublelist(train_data.review[i].decode(\"utf-8\"), tokenizer))\n",
    "            \n",
    "        train_list.append(clean_review(train_data.review[i], output_format=\"list\"))\n",
    "        if i%1000 == 0:\n",
    "            print \"Cleaning training review\", i\n",
    "       \n",
    "    if vector_type == \"Word2vec\":                \n",
    "        for i in xrange(0, len(unlab_train_data.review)):\n",
    "            word2vec_input.extend(review_to_doublelist(unlab_train_data.review[i].decode(\"utf-8\"), tokenizer))\n",
    "            if i%1000 == 0:\n",
    "                print \"Cleaning unlabeled training review\", i\n",
    "    \n",
    "    for i in xrange(0, len(test_data.review)):\n",
    "        test_list.append(clean_review(test_data.review[i], output_format=\"list\"))\n",
    "        if i%1000 == 0:\n",
    "            print \"Cleaning test review\", i       \n",
    "\n",
    "elif vector_type != \"no\": \n",
    "    for i in xrange(0, len(train_data.review)):\n",
    "        \n",
    "        # Append raw texts rather than lists as Count/TFIDF vectorizers take raw texts as inputs\n",
    "        train_list.append(clean_review(train_data.review[i]))\n",
    "        if i%1000 == 0:\n",
    "            print \"Cleaning training review\", i\n",
    "\n",
    "    for i in xrange(0, len(test_data.review)):\n",
    "        \n",
    "        # Append raw texts rather than lists as Count/TFIDF vectorizers take raw texts as inputs\n",
    "        test_list.append(clean_review(test_data.review[i]))\n",
    "        if i%1000 == 0:\n",
    "            print \"Cleaning test review\", i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using TFIDF\n",
      "Vectorizing input texts\n"
     ]
    }
   ],
   "source": [
    "# Generate vectors from words\n",
    "if vector_type == \"Word2vec_pretrained\" or vector_type == \"Word2vec\":\n",
    "    \n",
    "    if vector_type == \"Word2vec_pretrained\":\n",
    "        print \"Loading the pre-trained model\"\n",
    "        if model_type == \"bin\":\n",
    "            model = word2vec.Word2Vec.load(model_name)\n",
    "        else:\n",
    "             model = word2vec.KeyedVectors.load_word2vec_format(model_name, binary=True)\n",
    "\n",
    "    if vector_type == \"Word2vec\":\n",
    "        print \"Training word2vec word vectors\"\n",
    "        model = word2vec.Word2Vec(word2vec_input, workers=num_workers, \\\n",
    "                                size=num_features, min_count = min_word_count, \\\n",
    "                                window = context, sample = downsampling)\n",
    "    \n",
    "        # If no further training and only query is needed, this trims unnecessary memory\n",
    "        model.init_sims(replace=True)\n",
    "    \n",
    "        # Save the model for later use\n",
    "        model.save(model_name)\n",
    "    \n",
    "    print \"Vectorizing training review\"\n",
    "    train_vec = gen_review_vecs(train_list, model, num_features)\n",
    "    print \"Vectorizing test review\"\n",
    "    test_vec = gen_review_vecs(test_list, model, num_features)\n",
    "    \n",
    "    \n",
    "elif vector_type != \"no\": \n",
    "    if vector_type == \"TFIDF\":\n",
    "        # Unit of gram is \"word\", only top 5000/10000 words are extracted\n",
    "        print\"using TFIDF\"\n",
    "        count_vec = TfidfVectorizer(analyzer=\"word\", max_features=10000, ngram_range=(1,2), sublinear_tf=True)\n",
    "        \n",
    "    elif vector_type == \"Binary\" or vector_type == \"Int\":       \n",
    "        count_vec = CountVectorizer(analyzer=\"word\", max_features=10000, \\\n",
    "                                    binary = (vector_type == \"Binary\"), \\\n",
    "                                    ngram_range=(1,2))\n",
    "    \n",
    "    # Return a scipy sparse term-document matrix\n",
    "    print \"Vectorizing input texts\"\n",
    "    train_vec = count_vec.fit_transform(train_list)\n",
    "    test_vec = count_vec.transform(test_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing feature selection based on chi2 independence test\n"
     ]
    }
   ],
   "source": [
    "# Dimemsion Reduction\n",
    "\n",
    "if dim_reduce == \"SVD\":\n",
    "    print \"Performing dimension reduction\"\n",
    "    svd = TruncatedSVD(n_components = 200)\n",
    "    train_vec = svd.fit_transform(train_vec)\n",
    "    test_vec = svd.transform(test_vec)\n",
    "    print \"Explained variance ratio =\", svd.explained_variance_ratio_.sum()\n",
    "\n",
    "elif dim_reduce == \"chi2\":\n",
    "    print \"Performing feature selection based on chi2 independence test\"\n",
    "    fselect = SelectKBest(chi2, k=num_dim)\n",
    "    train_vec = fselect.fit_transform(train_vec, train_data.sentiment)\n",
    "    test_vec = fselect.transform(test_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform into numpy arrays\n",
    "if \"numpy.ndarray\" not in str(type(train_vec)):\n",
    "    train_vec = train_vec.toarray()\n",
    "    test_vec = test_vec.toarray()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "if scaling != \"no\":\n",
    "\n",
    "    if scaling == \"standard\":\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "    else: \n",
    "        if scaling == \"unsigned\":\n",
    "            scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "        elif scaling == \"signed\":\n",
    "            scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "    \n",
    "    print \"Scaling vectors\"\n",
    "    train_vec = scaler.fit_transform(train_vec)\n",
    "    test_vec = scaler.transform(test_vec)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM\n",
      "Optimized parameters: LinearSVC(C=10.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Best CV score: 0.87744\n"
     ]
    }
   ],
   "source": [
    "# Model training \n",
    "if training_model == \"RF\"\n",
    "    \n",
    "    # Initialize the Random Forest based model\n",
    "    rfc = RFC(n_estimators = 100, oob_score = True, \\\n",
    "              max_features = \"auto\")\n",
    "    print \"Training Random Forest\"\n",
    "    rfc = rfc.fit(train_vec, train_data.sentiment)\n",
    "    print \"OOB Score =\", rfc.oob_score_\n",
    "    pred = rfc.predict(test_vec)\n",
    "    \n",
    "elif training_model == \"NB\":\n",
    "    nb = naive_bayes.MultinomialNB()\n",
    "    cv_score = cross_val_score(nb, train_vec, train_data.sentiment, cv=10)\n",
    "    print \"Training Multinomial Naive Bayes\"\n",
    "    print \"CV Score = \", cv_score.mean()\n",
    "    nb = nb.fit(train_vec, train_data.sentiment)\n",
    "    pred = nb.predict(test_vec)\n",
    "    \n",
    "    nb = naive_bayes.GaussianNB()\n",
    "    cv_score = cross_val_score(nb, train_vec, train_data.sentiment, cv=10)\n",
    "    print \"Training Gaussian Naive Bayes\"\n",
    "    print \"CV Score = \", cv_score.mean()\n",
    "    nb = nb.fit(train_vec, train_data.sentiment)\n",
    "    pred = nb.predict(test_vec)\n",
    "    \n",
    "    \n",
    "elif training_model == \"SVM\":\n",
    "    svc = svm.LinearSVC()\n",
    "    param = {'C': [1e15,1e13,1e11,1e9,1e7,1e5,1e3,1e1,1e-1,1e-3,1e-5]}\n",
    "    print \"Training SVM\"\n",
    "    svc = GridSearchCV(svc, param, cv=10)\n",
    "    svc = svc.fit(train_vec, train_data.sentiment)\n",
    "    pred = svc.predict(test_vec)\n",
    "    print \"Optimized parameters:\", svc.best_estimator_\n",
    "    print \"Best CV score:\", svc.best_score_\n",
    "    \n",
    "    \n",
    "\n",
    "# Output the results\n",
    "if write_to_csv:\n",
    "    output = pd.DataFrame(data = {\"id\": test_data.id, \"sentiment\": pred})\n",
    "    output.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
